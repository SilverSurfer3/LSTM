import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Given data
data = [
    1, 1, 2, 1, 2, 2, 2, 3, 2, 5, 0, 0, 1, 2, 0, 2, 1, 2, 2, 1, 5, 0, 1, 0, 1, 2, 3, 2, 3, 0, 4, 2, 1, 3, 1, 3, 2, 1, 0,
    1, 2, 1, 3, 1, 1, 3, 2, 2, 2, 3, 2, 5, 3, 3, 2, 2, 1, 1, 1, 5, 2, 4, 0, 2, 1, 1, 3, 3, 2, 0, 1, 1, 1, 1, 2, 1, 0, 1,
    2, 2, 2, 2, 5, 3, 1, 2, 1, 0, 3, 0, 0, 0, 4, 2, 0, 0, 1, 0, 2, 0, 1, 2, 1, 1, 1, 4, 4, 0, 3, 2, 0, 5, 3, 1, 2, 4, 1,
    3, 1, 1, 3, 0, 2, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 3, 2, 2, 1, 1, 1, 1, 0, 0, 2, 3, 3, 3, 2, 1, 1, 5, 0, 2, 1, 2, 1, 0,
    1, 3, 2, 2, 3, 1, 0, 1, 0, 3, 1, 1, 1, 2, 0, 3
]

# Create a DataFrame for easier feature engineering
data_df = pd.DataFrame({'score': data})

# Feature engineering
data_df['previous_score'] = data_df['score'].shift(1)
data_df['rolling_mean'] = data_df['score'].rolling(window=3).mean()
data_df['rolling_std'] = data_df['score'].rolling(window=3).std()
data_df['score_diff'] = data_df['score'].diff()

# Exponential Moving Average (EMA)
data_df['ema'] = data_df['score'].ewm(span=3, adjust=False).mean()

# Percentage Change
data_df['percentage_change'] = data_df['score'].pct_change()

# Min, Max, Median
data_df['rolling_min'] = data_df['score'].rolling(window=3).min()
data_df['rolling_max'] = data_df['score'].rolling(window=3).max()
data_df['rolling_median'] = data_df['score'].rolling(window=3).median()

# Quantiles
quantiles = [0.25, 0.5, 0.75]
for quantile in quantiles:
    data_df[f'quantile_{quantile}'] = data_df['score'].rolling(window=3).quantile(quantile)

# Convert data to numpy array
data_array = np.array(data_df['score']).reshape(-1, 1)

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data_array)


# Create sequences for LSTM
def create_sequences(data, seq_length):
    sequences = []
    for i in range(len(data) - seq_length):
        sequence = data[i:i + seq_length]
        sequences.append(sequence)
    return np.array(sequences)


# Set the sequence length (adjust as needed)
sequence_length = 3

# Create sequences for training
sequences = create_sequences(data_scaled, sequence_length)

# Split into features and target
X = sequences[:, :-1]
y = sequences[:, -1]

# Reshape the data for LSTM input (samples, time steps, features)
X = X.reshape((X.shape[0], X.shape[1], 1))

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(50))
model.add(Dropout(0.2))
model.add(Dense(1))

# Compile the model with corrected learning rate argument
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
model.fit(X, y, epochs=100, batch_size=4, callbacks=[early_stopping], verbose=1)

# Set the number of days to forecast (adjust as needed)
n_forecast = 1

# Predict using the trained model
forecast = model.predict(X[-n_forecast:].reshape((n_forecast, sequence_length - 1, 1)))

# Rescale the forecasted values
forecast = scaler.inverse_transform(forecast)

# Print tomorrow's forecasted value
print("Tomorrow's Forecasted Value:", forecast.flatten()[0])

# Plot the original data and the forecasted values
plt.figure(figsize=(12, 6))
plt.plot(data, label='Original Data')
plt.plot(range(len(data), len(data) + n_forecast), forecast, label='Forecasted Data', color='red')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.show()

